
#script

##intro
안녕하세요. 이전시간에 이어서 계속 발표 진행하겠습니다.

##27-1 (1)
이번에 소개해드릴개념은 아이젠밸류와 아이젠벡터입니다. 
둘다 이제 행렬 A에 대한 아이젠벡터, 아이젠밸류 이렇게 쓰는것이고, A는 정사각행렬이어야 합니다.
Ax=람다x 를 만족하는 x를 A의 아이젠벡터, 그때 람다가 아이젠밸류입니다. 그리고 아이젠벡터는 nonzero 벡터입니다.

##27-2 (2)
예를들어 설명을 해보면, 요 [26][53]을 A라고 하면, 아이젠벡터 x=[1,1]을 구할수있습니다. 그리고 Ax=[8,8]이기 때문에 아이젠밸류는
8이 되는것입니다.

##27-3 (3)
그리고 이제 아이젠벡터를 사용하는 이유중 하나인데요, 바로 계산이 빨라지기 때문입니다. 두번의 곱셈만하면 되기때문이죠.

##27-4 (4)
그리고 다음은 이제 이 식Ax=람다x를 저희가 (A-람다I)x라고 쓸수 있습니다. 람다x를 람다I * x라고 쓸수 있기때문입니다.
아까 2653을 다시 생각해보면, 람다가 8이었기 떄문에 A-람다I를 -6 6 -3 3 이라고 쓸수 있습니다.
그리고 여기에 x벡터를 곱해서 0이 나오는데 x가 0벡터가 아닌, 즉 non trivial solution이 존재힌디는건 
A-람다I가 linearly dependent 하다 라는 뜻입니다. 왜냐면 lin indep이었으면, 0벡터외에 해가 존재하지
않기 떄문입니다.

##28-1 (5)
즉 A-람다I가 linearly dependent 해야하므로 그 뜻은 det가 0이되어야한다는 의미 입니다.
그리고 이 식을 characteristic equation이라고부릅니다.

##28-2 (6)
네 그래서 같은 예시로 람다값을 구해보는 과정입니다. 

##29-1,2 (7,8)
다음은 널스페이스 입니다.
네 A의 널스패이스 라는 의미는 Ax=0을 만족시키는 x의 집합입니다.
그러므로 A의 row들을 생각해보면, 이제 각 row들과 x가 orthogonal해야되는것입니다. 
예를들어서 A가 5*5매트릭스라고하고, A의 row들이 이제 평면, 즉 2차원을 span한다고 해보겠습니다.
이때 널스페이스를 생각해보면, 3차원공간을 span하게 됩니다. 그 이유는 5차원 벡터가 이제 
평면에 대해 수직이어야 하므로, 5-2헤서 3차원을 스팬하게 되는것입니다. 2차원에 대해서는, 평면에 수직이어야 하기 때문이죠.
그럼 null space도 subspace가 되겠죠. 
이는 subspace의 조건, 0벡터를 포함하고 linear combination에 닫혀있으면 되기때문에, 널스페이스또한 서브스페이스가 됩니다.

##29-3 (9)
그래서 이걸 orthogonal complement인데, complement가 보충한다라는 뜻이니까, 
아까의 예시에서 5차원을 이제 2,3으로 나눠서 갖게되잖아요? 그래서 orthogonal complement라고 하고
W와 Wㅗ 이렇게 씁니다.

##31-1 (10)
그럼이제 diagonalization 입니다.
이게 뭐냐면, diagonal matrix, 대각행렬인데 이는 대각선의 값을 제외하고 모두 0이어야합니다. 
이을 만들어주는것입니다.
그러니까 A라는 매트릭스를, V-1AV 를 통해 대각행렬로 만들어주는것입니다

##31-2 (11)
요 대각화는 아이젠벡터와 아이젠밸류와 연관이 있는데요, 아까의 예시에서 생각을 해보겠습니다.
A를 대각화시키려면, V의 열벡터를 아이젠벡터로 두면 됩니다. 그럼 
D가 이제 아이젠밸류러의 대각행렬이 되는것입니다.
그리고 이는 AV=AD로 다시 쓸수있겟죠.

##31-3 (12)
이걸 한번 보면, AV는 A [v1 v2 v3..] 이니까 [Av1 Av2.. ]로 쓸수있고,
VD는 이제 D가 대각행렬이기떄문에, [람다1v1 람다2v2.. ] 이렇게 됩니다.
그래서 AV=VD가 Av1=람다1v1 이렇게 열벡터끼리 같다로 생각해보면, v1은 아이젠벡터고 람다가 아이젠밸류기때문에,
이는 당연히 성립합니다.

##31-5 (13)
그럼 대각화가 가능하려면, V-1를 찾아야하므로 V가 선형독립일떄 가능합니다.
그 뜻은 A의 아이젠벡터들이 선형독립어야한다라는 말과 동치가됩니다. V가 A의 아이젠벡터 매트릭스이기 때문이죠

(여기부터 pdf)
##32-1 (14)
이번엔 eigendecomposition인데요, 이번엔 A를 이제 3개의 매트릭스의 곱으로
분해가 된것입니다. 그러니까 대각화가능하다는게 요 eigendecomposition이 가능하다 입니다.

##32-2 (15)
이번엔 리니어트랜스폼에서 생각을해보겠습니다.
Ax라는 변환에서 A를 VDV-1라하면, Ax는 이렇게 괄호로 합성함수로 표현할수있습니다.

##32-3 (16)
그럼 첫번째 변환이 V-1 x 가되는데.
먼저 예시를들어서 설명을 드리겠습니다. 일단 A의 아이젠벡터를 안다고 해봅시다. 
여기서 v1=[3,1],v2=[-2,1]가 됩니다. 그리고 입력벡터는 현재 [4,3]입니다.
이때 V-1x를 구해야하는데, y라고 두면, Vy=x를 만족하는 y를 찾는과정이라고 생각해도 되는겁니다.
그러면 v1,v2의 선형결합으로 x를 표현하는것이죠. 여기 예시에서는 평행사변형을 보면 [2,1]이 되는겁니다.
그래서 결국 V-1x가 [2.1]입니다. 

##32-4 (17)
다음은 이제 Dy입니다. D는 아이젠밸류를 갖는 대각행렬이므로, 행렬곱을 할필요없이 엘리멘트 와이즈하게, 
그냥 원소끼리만 곱해주면 됩니다. 그이유는 대각행렬이기 떄문입니다.

##32-5 (18)
그래서 이를 좌표평면에서 보시면 이전 계산이 벡터의 방향은 바꾸지 않고 아이젠밸류만큼 곱해서 크기만바꾼것이기 떄문에,
왼쪽그림처럼 나타나게 됩니다.

##32-6 (19)
그럼이제 마지막 연산입니다.
Vz가 되는데, V가 각각 아이젠벡터고 저희가 만든 z의 basis가 현재 v1,v2이기 떄문에, 
v1,v2의 선형결합으로 나타나기 때문에, 그 선형결합의 가중치들이 -2,2 이고
이를 원래의 베이시스로 복원한다고 생각할수있는겁니다. 
Vz는 이전 슬라이드에서의 두벡터의 합으로 볼수있는것입니다.

##32-7 (20)
네 그래서 두벡터의 합이 되어야하는것이죠

##32-8 (21)
그러면 첫번쨰 연산부터 다시 보겠습니다. 
첫번쨰 연산은 좌표계를 10 01에서 아이젠 벡터로의 새로운 베이시스로 변환해줬다고 생각하면 되는것이죠.
그러고 두번째는 D가 diagonal이므로 벡터에 아이젠밸류를 곱한것이고, 
마지막은 아까 말씀드린대로 원래의 베이시스로 복원한다고 생각하면 되는것립니다.

##32-9,10 (22,23)
그럼 이걸 
A의 k승 하는데 사용할수있습니다. D가 대각행렬이기때문에, 그냥 Dk승을 구하면 되기때문입니다.
계산하는 시간이 엄청 줄어드는 이점을 볼수있는것입니다

##33-1 (24)
다음은 시메트릭하다 입니다.
시메트릭하다는건 정사각행렬애만 적용되는것이고. 대각선을 대칭으로 값이 같아야하는겁니다.
그 뜻이 AT=A인것이죠. 그래서 AAT, ATA가 시메트릭합니다. (AT)T가 A이기 때문이죠

##34-1 (25)
그러면 시메트릭 매트릭스의 성질을 알아봅시다.
먼저 A가 시메트릭하다 하면, 서로다른 아이젠스페이스에서 온, 즉 서로 다른 아이젠밸류를 갖는 어이젠벡터끼리는 직교합니다. 
일단 서로다른 아이젠밸류 람다1 람다2 가 있다하고, 그에따른 아이젠벡터 v1v2가 있다고하면,
v1,v2가 선형독립이 되어야합니다. 
증명을 보시면 먼저 람다1v1 과 v2의 내적을 봅시다 그러면 이를 (람다1v1)T v2 가 되는것이죠 그럼 이를 (Av1)T v2로 쓸수있고
정리하면, 람다2v1 내적 v2가 됩니다. 근데 람다1-람다2는 0이 아니니까, v1과 v2가 내적해서 0이 되어야하는겁니다.

##35-1 (26)
네 그리고 orthogonally diagonalizable 하다는 의미는, A를 PDP-1로 나타낼수있고, P가 orthonormal한 백터들인것입니다.
P-1 = PT 이기 떄문에, PDPT 이렇게도 쓸 수 있습니다. 결론적으로 A가 시메트릭하면, P-1 = P.T라 PDP.T로 아이젠 디컴포지션이가능하고 
orthogonally diagonalizable라고 합니다

##41-1 (27)
다음은 스펙트럴 디컴포지션입니다.
이는 그냥 시메트릭 매트릭스의 아이젠디컴포지션입니다.
A=PDPT로 나타낼수있고, PD먼저 보면 D가 diagonal이므로 이렇게 [람다1u1, 람다2u2,,,] 로 나타낼수있습니다.
이후 PT와 행렬곱인데 이는 sum of outer product 로 생각할수있죠.

##41-2 (28)
그래서 이제 A를 PDPT의 행렬곱에서 덧셈으로 나타낼수있는겁니다.
이를 스팩트럴 디컴포지션이라하고, 각항을 스펙트럼이라고 합니다. 

##41-3,4 (29,30)
이를 예시를 통해 볼 수 있습니다. 

##41-5 (31)
다음은 spectral theorem 입니다.
a에서 보시면 시메트릭한 A는 먼저 n개의 실수인 아이젠밸류을 갖습니다. 이 뜻은 특성 방정식이 n개의 실근을 갖는다는 뜻입니다.

##41-6 (32)
결론적으로 이 A의 서로다른 아이젠벡터들이 다 orthogonal하고, 
아까 말씀드린대로 orthogonally diagonalizable 입니다.

##43-1 (33)
네 그러면 이제 SVD에 대해 알아보겠습니다.
지금까지는 모두 정사각행렬에서만 정의된건데, SVD는 이제 일반적인 매트릭스에서도 적용되는것입니다.
그러니까 A를, U, sigma, V.T로 분해하는것입니다.
U와 V는 각각 orthonormal합니다. 그래서 V.T는 row들이 orthonormal합니다.
그리고 가운데 시그마는, eigendecomposition과 비슷하게 대각선의 원소를 제외하고 0이라고 생각해주면 됩니다.

##43-2 (34)
그래서 그림을 보시면
이걸 어떻게 생각할수있냐면, U와 V.T의 outer product의 합인데, 가운데의 diagonal matrix의 원소만큼
scaling된다 생각하시면 됩니다.

##43-3 (35)
그래서 즉 n개만큼의 outer product의 합이되는것입니다.

##43-4 (36)
네 그래서 이걸 아래에 있는것처럼 reduced form으로 나타낼수있는것입니다. 이러면 u가 m*n matirx가 됩니다.

##44-1 (37)
그럼 SVD를 구할떄는, ATA부터 시작합니다. 
A가 m*n이면, ATA는 n*n의 시메트릭 행렬입니다.
그전에 싱귤러 밸류에 대해 알아보겠습니다. 싱귤러 밸류란, 아이젠밸류의 루트를 씌운값입니다.
ATA의 아이젠밸류의 루트를씌운값이 A의 싱귤러밸류 입니다. 그런데 nonzero여야 하는겁니다.
그러니까 r개만큼의 싱귤러 밸류라고하면, 즉 n개중 r개만큼만 0보다 크므로 r<=n이렇게 될수밖에없는것입니다. 
그리고, v1,v2...vn이 이제 ATA의 아이젠벡터였는데, 이중에 아이젠밸류가 0보다 큰, 그러한 아이젠밸류와 쌍이되는
아이젠벡터에 다시 A를 곱하는겁니다. 그러면 Av1,Av2..Avr은 n차원에서 m차원이 될것이고, 이 r개의 벡터가 
서로 orthogonal 하고 colA의 basis가 됩니다.

자 그러면 증명을 보겠습니다.
먼저 Avi, Avj를 내적시켜봅니다. 이값이 0 이되면 되는것이죠.
그러면 (Avi)T * Avj 가 되고 이는 viT ATA vj이렇게 됩니다. 그런데 ATAvj 가 람다jvj이므로, 
이는 vi와 vj가 orthogonal하므로 0이됩니다. 

##44-2 (38)
다음은 벡터의 길이입니다. ||Av1||^2을 생각해보면 (Av1)T(Av1)이므로 이는 v1TATAv1이고 이는 그냥 람다1이 됩니다.
즉 ||Av1||이 루트 람다1이 되므로, 길이는 싱귤러 밸류와 동일합니다. 
그레서 만약에 ATA에서 람다가 0이 되는 아이젠벡터가 있을때 그 아이젠벡터에 A를 곱해주먄 0벡터가 되는것이죠.

네 그럼 colA에 속하는 어떠한 벡터y를 표현해볼건데 y=Ax라하고, x를 v1,v2,v3..를 베이시스로 해서 표현을 해보겠습니다. 그럼
x=c1v1+c2v2...cnvn이렇게 표현됩니다. 
그럼 y=Ax 가 c1Av1+v2Av2...+crAvr...+cnAvn입니다. 근데 r이후의 Avr+1 이런값은 0벡터가 되므로, 
모든 y는 Ac1,Ac2...Acr의 linear combination이 되는것입니다. 즉 모두 orthogonal 하고 선형독립하므로 이를  
col A의 basis라고 생각할수있습니다. 그래서 dim colA = r이 되어서, A의 rank = r입니다.

##44-3 (39)
네 그 증명입니다.

##45-1 (40)
네 그래서 SVD theorem은 A가 m*n일때 반드시 SVD분해가 가능하다 입니다.

##45-2 (41)
그리고 시그마의 대각선 엔트리는 모두 0보다크고, u를 left singular vector, v를 right singular vector라고 합니다.

그럼이제 증명을 보겠습니다.

##45-3 (42)
먼저 normalize합니다. 즉 Av1,Av2..Avr 를 싱귤러밸류로 각각 나눠주면 됩니다. 그러면 Avi=sigma i ui이 됩니다
그럼 u1,u2,,ur이 되고, 이를 이제 m차원의 basis로 만들어주기 위해 확장을 해보겠습니다. 그램슈미트프로세스를 이용하면,
이때 r개의 벡터는 orthonormal합니다.
그러면 확장하면 U가 u1,u2...um 이 됩니다. 이는 모두 orthonormal합니다. 

##45-4 (43)
네그러면 AV가 [Av1,Av2,,,Avr,0,0,0...] 이 됩니다. 그리고 A가 m*n, V는 ATA의 아이젠벡터들이므로 n*n이죠.
AV는 m*n이 됩니다. 이는 [sigma1 u1, sigma2 u2...0...] 이 됩니다.
그러면 이걸 U * sigma 로 나타낼수있는것이고, 이때 U가 m*m, sigma가 m*n 이 됩니다.
그럼 아래와 같이 U sigma 가 AV랑 일치하게 되는것입니다.

그러면 V.T를 곱하여, A=u sigma V.T가 되는것입니다.
