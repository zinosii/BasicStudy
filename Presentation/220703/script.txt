#script

##intro
안녕하세요. 이번 스터디에 선형대수파트 발표를 맡게된 박진호 입니다.
여러분들 모두 교수님의 강의를 듣고 오셨다는 가정하에 발표 진행하겠습니다.

첫번째 발표는 교수님 강의 8강부터 18강 까지 제공되는 슬라이드로 발표 진행하겠습니다.

##8-1
먼저 Linear combination, 즉 선형결합입니다. 예를들어 R3 의 공간이 있다고 가정하고, 총 p개의 3차원 벡터v1,v2,..vp가 존재한다고 하면, 그것들의 선형결합을 우리가 c1v1+c2v2.. 이렇게 쓸 수 있는것입니다.
이 때 c1,c2.. 를 가중치 또는 계수 라고 합니다. 
네 그리고 이 가중치들은 0을 포함한 실수가 될 수 있습니다.

##8-2
네 그리고 다음은, 여기 각각의 row들이 하나의 방정식인데, 이를 column으로 보는것입니다. 즉 첫번째 칼럼은 키에 대한 벡터, 두번째는 키, 세번째는 흡연여부 이렇게 생각 할 수 있는것이죠.
그리고 이 벡터들의 선형결합을 생각해볼수있는데요, 그것이 바로 이 식 입니다. 제가 생각하는 중요한 점은 처음 주어진 세개의 방정식을 이렇게 Matrix equation과 vector equation으로 표현할수 있는것이고, 두표현이 동일한 것입니다. 

##8-3 
그럼 이 벡터 방정식에서 해가 존재하는지가 중요한데 그걸 알기 위해서 이 span이라는 개념을 알아야합니다.

##8-4
그럼 이 span이 뭐냐면, 주어진 벡터들로 만들어질수 있는 모든 선형결합의 집합이라고 생각하시면 됩니다.
예를 들면 두개의 삼차원벡터 v1 = {1,2,3} , v2 = {4,5,6} 이 있다고 가정하면 이 두개의 벡터로만들어지는 모든 선형결합의 집합이 span{v1,v2}가 되는것 입니다.
그럼 모든 실수 c1,c2에 대하여 c1v1+c2v2이기 때문에, {1,2,3}, {5,7,9} {0,0,0} 등등이 이 span{v1,v2}의 원소가 되는것입니다. 이것이 span의 정의입니다.

##9-1
그럼 공간상에서 이 span을 생각해볼수 있겠죠. 
전의 예시 그대로, v1={1,2,3} v2={4,5,6}이라고 해봅시다. 그럼 이 x=2v1+3v2는 v1과 v2의 선형결합이고, span{v1,v2}에 포함이 되겠죠? 그리고 벡터의 합이므로, 이 벡터x가 점 x가 되는것입니다. x가 벡터인데, 공간상의 점으로 표현될수 있는것이죠. 그러면 이 모든 점들을 모아보면, 바로 두개의 벡터로 이루어지는 평면이되는것을 알 수 있으실겁니다.
추가로 생각해볼수있는것이, 하나의 벡터의 span입니다. 예를들면 span{v1}이라고 해보죠. 그러면 c1v1의 집합이므로, 그냥 직선이되는것입니다. 

##9-2
그럼다시 벡터방정식으로 돌아가봅시다. 이 벡터방정식의 의미는, 요 세개의 벡터의 선형결합, 즉 상수 벡터 b가 span{v1,v2,v3} 에 포함이 될때, 이 x1,x2,x3 즉 가중치들을 구하는 과정이라고 생각하시면 되는것입니다. 이 가중치들은 각 벡터 v1,v2,v3를 얼마만큼 늘리느냐? 라고 생각하셔도 되는것입니다.
그래서 벡터방정식의 해가 존재하려면, 반드시 이 b벡터가 요 벡터v1,v2,v3들의 span안에 들어가있어야하는것입니다.

여기서 추가로 생각해볼수있는게, 만약 세개의 벡터가아닌 두개의 벡터만 주어졌다고 생각해보면, 해가 존재하지 않을수도 있는것입니다. 뭐 운이 좋게도 v1,v2평면위에 존재하는 경우면 해가 존재하지만, 두개의 벡터만 존재하는 경우에는 3차원 공간이 표현되지 못하는것입니다. 
그럼 다시 요 방정식에서 생각해보면, 벡터의 차원이 방정식의 갯수가 되는것이고, 요 span의 재료벡터의 갯수가 미지수의 갯수가 되는것입니다. 여기서는 키 몸무게 흡연여부 즉 세개의 미지수를 가지므로, span의 재료 벡터는 3개가 되는것이죠. 그래서 만약 방정식의 갯수가 미지수의 갯수보다 많아진다면, 대부분의 경우 해가 존재하지 않게되는것입니다.

##10-1
그리고 이번에는 matrix multiplication을 벡터들의 선형결합으로 생각해보는것인데요, 먼저 요 기본적인 행렬곱을 생각해봅시다.
3*2와 2*2 매트릭스이 곱인데, 이때 결과 매트릭스는 보시는것과같이 3*2 매트릭스가 됩니다. 그럼 총 6개의 원소들이 있는데, 이 원소들을 각각 따로따로 생각하면, 각각 row와 column의 내적으로 생각할수 있습니다. 예를들면 2번째 row와 1번째 col의 내적이 결과매트릭스의 (2,1)번째 요소가 되는것이죠.

이걸 좀더 간결하게 생각해볼수있는데, 아래 예시를 보시면, 이 행렬곱을 cloumn벡터들의 선형결합으로 생각할수 있는것입니다. 

##10-2
이게 무슨뜻인지 예시로 살펴보겠습니다. 
여기 3*3 matrix와 3*1 matrix의 곱에서, 결과가 3*1의 matrix가 되겠죠. 원래 행렬곱을하면 각각의 row들과 벡터를 내적시켜서 구해야 했으나, linear combination을 이용하여 3*3 행렬의 3개의 column벡터와 3*1행렬의 요소의 값을 가중치로 써서 선형결합으로 나타낼수 있는것입니다. 그리고 이 linear combination은 행렬곱의 결과와 당연히 일치하게 됩니다. 또 하나 알 수 있는것이, 이 결과값은 당연히 세개의 벡터의 span안에 포함되게 되는것입니다.
이 예시는 매트릭스와 벡터의 곱이었지만, 옆에 예시는 이를 행렬곱으로 확장시켜서 생각할 수 있는것입니다.

##10-3
이번에는 그럼 이 선형결합을 row들의 선형결합으론 볼수없을까? 입니다.
위의 예시를 보시면, 1*3행렬과 3*3행렬의 곱이므로 결과는 1*3행렬이 될것입니다. 이때 결과는 1*3행렬의 각요소를 가중치로보고, 3*3행렬의 row들의 linear combination으로 나타낼수 있는것입니다. 이번에도 마찬가지로 아래와 같이 확장될수 있습니다. row combination과 column combination에서 이는 (A*x)^T를 생각해보시면, 당연하다는것을 알수 있으실겁니다.

##10-4
그럼 마지막케이스인데요, 우리가 지금까지 행렬과 벡터의 곱에서 시작하여 행렬과 행렬의 곱으로 확장한것처럼, 이번에도 위의 개념을 확장시켜 보겠습니다.
보시면 column과 row의 곱인데요, 이 rank1 outer product를 이용해, 아래와 같이 쓸수있는것입니다. 여기서 중요한것이, 각각 벡터들의 차원은 달라도 되지만, 갯수는 같아야 한다는것입니다. 그래서 결론은 outer product의 sum으로 표현가능하다 입니다. 
다시 아래의 예를 보시면, 행렬을 각각 row단위와 col단위로 분해하여 outer product의 합들로 나타낼수 있는것입니다.

##10-5
네 그래서 style transfer같이 머신러닝에서 많이 사용되고 있다 라고 생각하시면 됩니다.

##11-1
그럼 다시 연립방정식으로 돌아와보겠습니다.

##11-2
일단 Ax=b에서, b가 span안에 들어가있다고 생각해보겠습니다. 그럼 해가존재하는데, 이게 유니크한지 아닌지도 판단을해야겠죠.
이를 판단하려면, 선형독립인지 아닌지를 판단하면 되는것입니다.

##11-3
이게 무슨 뜻이냐면, span에 쓰이는 재료벡터v1,v2,v3...가 있을건데, 새로운 벡터 vp 를 재료벡터로 넣는다고 생각을 해볼게요. 이 때 만약 새로운 벡터가 이전 벡터들의 span에 포함 되지 않으면, v1,v2...vp는 선형독립이되는것입니다. 반대로 span안에 포함이 되면, 선형의존이 되는 것이죠. 즉 새로운 벡터가 이전벡터들의 선형결합인지 아닌지를 판단하시면 되는것입니다.

##11-4
이전에 definition이 이해하기 쉬운 definition이고 이번엔 좀 formal한 definition인데요, 정의를 보시면 v1,v2...의 선형결합이 있고, 그 값이 0이라고 해봅시다. 그럼 자명하게 모든 가중치가 0이면 반드시 성립하겠죠. 여기서 근데, 모두0이 아닌 다른 솔루션이 있다면, 이경우 선형의존이 되는것이고, 유니크한 0벡터만을 해로갖는다면 선형독립이 되는것입니다.

##11-5
그리고 이걸 풀어서 한번 설명드릴게요. v1,v2..vp가 선형의존일때, 마지막 벡터를 vj라고 해볼게요.
이 때 x1v2+x2v2+ ... +xjvj = 0 이라고 쓸 수 있고, 이를 vj에 대해 풀면 vj = -x1/xj * v1 .. 이 되는것이죠.
이게 무슨말이냐면, vj벡터가 v1..vj-1의 선형결합이므로, 이전벡터들의 span안에 들어간다는것이고, 이것이 즉 선형의존이 되는것입니다. 

##12-1
선형독립을 그림으로 생각을 해보면, v1,v2가 있을때 v3를 생각해볼게요. v3가 span{v1 v2}안에 있다고 생각하면, 그림에서 보시다싶이 평면위에 존재하게 됩니다. 그러면 이 때 v1v2v3가 선형의존이 되는것이고, 여러개의 솔루션이 나올수 있는것입니다.
이걸 예를들어 생각해볼게요. 지금 v3=2v1+3v2잖아요? v1=[1,2,3] v2=[4,5,6] 이므로 v3=[14,19,24]이렇게 되고, 
Ax=b에서 b를 [14,19,24]라고하면 가중치를 2 3 0 으로 주어도 만족하고, 0 0 1로 주어도 만족하므로, 유니크한 해가 존재 하지 않게 되는 것입니다. 
또 생각해볼수 있는것이, 선형독립이면 어떤점을 표현할떄 최소한의 벡터로 표현할수 있겠다 생각할수있는것이죠. 그리고 그것이 유니크한 해가 되는것입니다.

##12-2
그래서 보시면 v3가 span{v1,v2}에 포함되면, span{v1,v2,v3}가 되어도 span이 증가하지 않는것입니다.

##12-3,12-4
네 예시인데 읽어보시면 충분히 이해하실수 있을것같고요

##12-5
네 결론적으로 b가 span안에 있어야만 해가 존재하는것이고, 벡터들이 선형독립일때만 유니크한 해가 존재하는것입니다.

##13-1
이번엔 subspace에 대해 알아봅시다
subspace는 span과 유사한데요, span으로 나온 벡터들의 부분집합이라고 생각하시면되는데, 중요한건 선형결합에 대해 닫혀있는것입니다. 예를들어 R^3의 부분집합을 생각해볼게요 {[1,2,3],[4,5,6]} 은 subspace가 될수없습니다. 에초에 [5,7,9]가 저 집합안에 없으니까요. 하지만 span을 붙여 생각해보면, 당연히 subspace가 되는것입니다.

##13-2
subspace하면 basis가 빠질수없는데요, 기저벡터라고 합니다, 이를 만족하기 위해서는 선형독립이고, basis벡터들로 span이 subspace를 모두 커버하면 되는것입니다.

##13-3
그럼 basis가 유니크한지 생각해볼수있겠죠. 정답부터 말씀드리면, 기저벡터는 유니크하지 않습니다. v1,v2이 basis라면 v1+v2, v1-v2또한 기저벡터가 되고, 이러한 예시는 수없이 많습니다. 하지만 중요한건, basis의 길이는 모두 동일해야 하는것입니다. 그리고 그 갯수를 subspace의 디멘션이라고 합니다.

##14-1
그러니까, basis는 유니크하지 않지만, 갯수는 유니크하다는것입니다. 그리고 이 갯수를 dim으로 표현합니다.

##14-2
이번엔 column space에 대해 알아보겠습니다. 
이 예시에서 colA라는것은, A행렬의 column벡터들의 span입니다. 이는 subspace가 됩니다.
이것을 column space라고 합니다. 그럼 dim colA 를 생각해볼수있겠죠.

##14-3
이 예시에서 세번째 col벡터는 첫번째와 두번째벡터의 합으로 나타낼수 있으므로, 즉 linearly independent하지 않으므로 dim colA는 3이아니라 2가되는것입니다.

##14-4
그럼이제 rank를 알수있는데, rank A란 dim colA 가 되는것입니다.
이전에 rank1 outer product를 배웠었는데, 하나의 column만 있었으므로 당연히 rank1이 되는것입니다.

##15-1
이번에 배워볼것은 변환입니다
Ax=b에서, A를 5*3행렬, x를 3차원 벡터라 생각해봅시다. 이 때 b는 5차원 벡터가 되겠죠. 이때 A를, 3차원벡터를 5차원벡터로 변환해주는 함수라고 생각해보는것입니다.
이때 여러 용어들이 있는데, Domain,즉 정의역 그리고 codomain, 공역입니다. 도메인의 모든원소는 오직 하나의 코도메인의 원소로 매핑되어야합니다. 
그리고 range,즉 치역 입니다. 그리고 image는, 특정인풋의 아웃풋을 의미하는데요, 즉 2에 대한 이미지는 5다 라고생각하시면 됩니다. 모든 이미지를 모아놓은것이, 바로 range가 되는것이죠

##15-2
그럼 linear한 transformation은 뭔지 생각해봅시다.
linear하다의 의미를 봐야하는데요. 그 조건은 이런겁니다. 입력두개가 있다고 해볼때, 그 입력값을 따로따로 함수에 넣으면, 어떠한 출력값이 두개가 나오겠죠 그 출력값들의 선형결합과, 인풋을 먼저 선형결합해서 하나의 인풋으로 보고 나온 결과값이 같은경우를 linear하다 합니다. 
예시를보시면 이해가 되실겁니다 (예시설명하기)

##16-1
이번엔 정의역을 R3, 공역을 R2라고 해봅시다. 이러면 인풋벡터는 모든 3차원 벡터가 되는것이죠. 예시를 봐도 똑같습니다.

##16-2
예제를 통해 선형변환을 보겟습니다.
예시를보시면, R2->R3이므로 2차원벡터를 3차원벡터로 변환하는것이죠.
먼저 x, 2차원 벡터이므로 [x1,x2]라고 쓸수 있고, 이를 x1*[1,0] + x2*[0,1]이라고 해봅시다.
그럼 T(x)를 [2,-1,1]과 [0,1,2]의 선형결합이라고 할 수 있으므로, 이를 매트릭스로 만들어서 Ax 꼴로 만들수있는겁니다.

##16-3
이 A의 매트릭스를, 스탠다드 매트릭스라고 합니다.

##16-4
새로운 예시를 보시면, 이번엔 R3->R2입니다. 이 예시또한 이전 예시와 같이 standard matrix를 구할수 있겠죠. 여기 [1,0,0][0,1,0][0,0,1] 이 3차원공간을 나타내는 가장 기본적인 벡터잖아요, 이를 standard basis라고 합니다. 그래서 이 매트릭스를 스탠다드 매트릭스라고 하는겁니다.

여기서 추가적으로 설명드리고싶은것이 있는데, 요 R2->R2예시에서설명을 해보겠습니다.
[1,0] -> [3,1] [0,1]->[2,5] 라고 하면 선형변환의 스탠다드 매트릭스는 [[3,1],[2,5]]가 됩니다.
그러면 [2,3]벡터는 어떻게 변환이 되는지 생각해보는겁니다.
이제 정의역 죄표평면에서 이를 살펴보면, [1,0]을 2배, [0,1]을 3배 만큼 이동하였으므로 (2,3)격자가 될것입니다.
이를 [3,1], [2,5]로 변환하여 생각해보면 옆에 좌표평면처럼 나타낼수 있겠죠. 그리고 여기서 [2,3]의 변환은, 평행사변형을 그려 쉽게 눈으로 확인해볼수있습니다. 즉 제가 말씀드리고싶은것은, 이 왼쪽의 (1,1)격자점이 여기 공역의 좌표계에서는 이렇게 변하는것입니다.

##17-1
이번에는 이 행렬에의한 변환이 어디서 쓰이는지 알아보겠습니다.

##17-2
먼저 사진이 인풋으로 들어왔을때, cat/dog/ship 으로 분류하는 뉴럴넷을 생각해보겠습니다
이때 인풋 벡터는, 고양이사진의 픽셀입니다. 현재 2*2이미지이므로, 4*1 벡터로 만드는것입니다. 이것이 입력벡터입니다
그리고 아웃풋노드는 세개가 되는데, 각각 cat/dog/ship입니다 그러면 4차원벡터가 3차원벡터로 변환되는것이기 때문에, 요 변환을 3*4매트릭스의 곱으로 생각할수있는것입니다. 그리고 보통 결과값인 3차원벡터에 bias, 여기서는 [1.1,3.2,-1.2]를 더해줍니다. bias는 모델을 조금 flexible하게끔 해준다고 생각하시면 됩니다.

이미지가 들어왔을때, 뉴럴넷의 학습되는 방법은, 요 3*4행렬의 가중치값과 상수값을 계속 바꿔주는것입니다. 즉 고양이사진이 인풋이므로 고양이에 해당하는 결과값이 커지고, 나머지는 작아지도록 학습하는것이죠. 각각을 score로 생각하시면 되는것입니다. 
bias의 합은 인풋에 1을 하나 추가시켜서, 아래와 같이 바꿔줄수있습니다. 위에보시면 Affine layer인데, bias의 존재로 linear하지 않기 때문에 Affine layer라고 합니다. 그러나 아래처럼 1을 추가시켜 linear로 바꿀수있는것입니다.

##18-1
이번엔 onto와 one-to-one입니다.
onto란, 한국어로 전사라고하고, 공역과 치역이 같은경우 입니다. 즉 모든 공역의 원소가 함숫값으로 다쓰였다라는것입니다. 그러므로 치역과 같은것이죠.

##18-2
onetoone은 1대1입니다.
치역의 원소하나하나 생각할때, 그 치역의 값이 되는 정의역의 원소가 있을것인데, 그 원소가 오로지 하나인경우입니다. 즉 두개이상의 정의역원소가 하나의 함숫값이되면, 1대1이 아닌것이죠.

##18-3,4
네 그럼 뉴럴넷관점에서 보시면, 몸무게 키 흡연여부가 있고, 투레이어를 생각해보면 먼저 R3->R2가 되고 R2->R1의 선형변환인것입니다. 각각의 레이어는 션형결합이라고 생각하시면 됩니다. 즉 오버웨이티드는 예를들어 weight*2-height+30 이런식으로 생각할수있는것입니다. 여기서 1대1과 onto를 생각해봅시다.
만약 결과가 78세 라고 생각해봅시다. 만약 1대1이었으면, 특정 몸무게 키 흡연여부만이 78세로 나오는것입니다. 하지만 1대1이 아니면, 다른 feature값으로도 78세가 나올수있는것입니다.

##18-5 이번엔 onto관점에서 보면, 공역은 0~10000인데 저희 학습데이터를 보면 55세에서 88세라고 하면, 55~88세로 결과값을 매핑해주어야 하는것이죠. 라이프스팬이 가능한 값은 0~10000이지만, 55~88로만 매핑이된다하면 onto가 아닌게 되는것이죠. 

##18-6
네 그러면 이 개념들을 해가 존재하거나, 존재할때 유니크하거나의 개념으로 확장시켜보겠습니다.

##18-7
예시를 통해보시면, 일단 요 변환은 R2->R3이 됩니다.
여기서 one to one, onto를 생각해봅시다.
일단 onto가 되려면 공역과 치역이 같아야하는데, 이경우 R2->R3이므로, 2차원공간의 점의개수보다 3차원공간의 점의 개수가 훨씬 크므로 onto가 될수 없는겁니다.
그러면 이번엔 one to one에 대해 생각해보면, 치역의 원소로 매핑되는 정의역의 원소가 유니크하냐를 따지면 되겠죠.
이건 linearly independent개념과 연결이 되는데, A의 column벡터들을 생각해보면 됩니다. 선형의존이되면 해가 무수히 많아지므로 여러개가 매핑이 되고, 선형독립이어야만 one to one이 되는것이죠

##18-8
그럼 이번엔 R3->R2를 생각해봅시다.
아까와 같이 생각해보면 이번엔 R3->R2이므로, onto가 됩니다. 그런데 만약 세개의 벡터가, 하나의 벡터로 일치한다고 생각해봅시다. 그러면 onto가 될수없겠죠. 그러므로 항상 R3->R2로 간다고 onto가 되지는 않습니다.
그리고 이번엔 세개의 벡터가 선형의존이기때문에, one to one이 될수는 없는것입니다.




