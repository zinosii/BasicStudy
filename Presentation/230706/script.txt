
#script

##intro
안녕하세요. 이전시간에 이어서 계속 발표 진행하겠습니다.

##19-1 (1)
이번에 알아볼개념은 요 over determined linear system 에서의 해 입니다
저희가 알다싶이 변수의 갯수보다 방정식의 갯수가 많으면, 일반적으로 해가 존재하지 않겠죠
이때 해를 근사적으로 찾아보는것이, least squares입니다.

##19-2,3 (2,3)
말씀드린대로 방정식, equations가 많아야 하므로. 요 3*3 매트릭스를 100*3으로 확장 시켜봅시다. 그럼 b벡터 또한 100차원 벡터가 되겠죠.
그리고 매트릭스는, 100차원 col벡터 3개가 되는것이죠. 여기에 3*1벡터를 곱해주면, 100차원을 커버하기엔 무리가 있는것이죠.
대부분의 b는 스팬 밖에 존재해서 해가 없게 되는겁니다.

##19-4 (4)
그럼 근사적으로 해를 찾으려는데, 근사적인 해가 무엇인지 알아봐야합니다.

##19-5 (5)
그전에 기본적인 개념을 알아봅시다. 
먼저 inner product입니다.
두 벡터의 내적, 즉 u * v 이렇게 쓸 수 있고, uT * v 라고 생각하시면 됩니다.
모두 아시다싶이 결과는 스칼라 값이 나옵니다.

##19-6 (6)
내적의 성질인데요, 교환법칙과 분배법칙이 성립합니다. 
그리고 요 상수c를 먼저곱하든, 나중에 한번에 곱하든 결과는 같습니다.
마지막 성질을 보기전에, 벡터의 길이먼저 설명드리겠습니다. 벡터 u가 있을때, u의 길이를 저희는 루트 u*u이라고 하고, ||u||라고 쓸수 있습니다.
그럼 uT * u 가 길이의 제곱이 되겠죠. 그럼이제 마지막 성질을 이해 하실 수 있을것 같습니다. 자기자신과 내적해서 0이기때문에,
그 벡터는 무조건 0이상이고, 0인경우는 무조건 0벡터가 되어야하는것입니다.

그리고 마지막 성질은, b,c성질을 이용한것입니다.

##20-1 (7)
네그럼 아까 설명드린 길이, 그 길이를 이제는 norm이라고 합니다.

##20-2 (8)
이 norm을 좀 기하학적으로 생각해보면, 왜 길이라고 하는지 알수 있으실겁니다.

##20-3 (9)
다음은 단위 벡터인데, 단위 벡터는 길이가 1인 벡터입니다.
여기 벡터 노멀라이징이라고 되어있는데, 벡터를 단위 벡터로 만들어 주는 과정이라고 생각하시면 됩니다.
즉 벡터 v를 norm으로 나눠주는것입니다. 그러면 방향은 같으나, 길이는 당연히 1이됩니다.

##20-4 (10)
그러면 이번엔 벡터간의 거리를 정의해보겠습니다.
이걸 좌표평면에서 보시면 이해가 빠를것같은데, 요 좌표평면상 두개의 벡터 u,v가 있다고 했을때, u,v사이의 거리를 정의해보면,
||u-v||가 되는데, 이를 u+(-v) 이렇게 생각해볼수 있고 이를 좌표평면에 나타내면 이 벡터가 되는것입니다. 요 평행사변형을보시면 이해가 빠르실겁니다.

##20-5 (11)
Rn에서도 마찬가지 입니다.

##20-6 (12)
다음은 두 벡터사이의 각도입니다.
두 벡터의 내적은, 두벡터의 norm의 곱과 두벡터사이 각의 코사인값의 곱이됩니다.
즉 cos theta 는 내적값을 norm의 곱으로 나눠준것과 같은것이죠.

##20-7 (13)
그럼 두개의 벡터가 수직인지를 이를 이용해 알 수 있겠죠.
내적해서 0이된다는 뜻은, 즉 코사인값이 0이되야하므로, 두벡터사이각이 90도, 즉 수직인 경우 입니다.
그래서 내적해서 0인지만 판단하면 되는것이죠. 물론 non zero 벡터여야 합니다.

##21-1 (14)
그럼 다시 아까의 방정식으로 돌아와볼게요.
우리가 이 방정식의 해를 이렇게 구했다고 해봅시다.

##21-2 (15)
그 후에, 새로운 네번째 식을 추가하는겁니다.
그러면 Ax=b 에서 A가 4*3 으로 바뀌겠죠. 또한 결과 값도 4*1 벡터로 바뀌게 됩니다. 그러면 이 마지막 원소는, 72와 같다는 보장이 없습니다.
즉 에러가 발생합니다. 왜냐면 x가 마지막 방정식에 대해서는 고려하지 않은 솔루션이기 때문입니다. 
그리고 에러는, b-Ax라고 할 수 있습니다.

##21-3 (16)
그럼 이번엔 다른 솔루션을 대입해보겠습니다.
그러면 저희 모델에 예측치는 이렇게 나오겠죠. 에러도 당연히 달라지는겁니다.

##21-4 (17)
그럼 더 나은 솔루션은 뭐가 될지 생각해볼수있겠죠.

##21-5 (18)
그걸 판단하기 위해 요 새로운 식, sum of squared errors 를 생각해볼수있습니다.
위의 에러가 더 작기때문에, 위의 솔루션이 더 나은 솔루션이라고 할 수 있습니다.
그리고 이식은 b-Ax의 norm입니다.

##21-6 (19)
그럼 이 에러를 최소화 하는것을 생각할수 있겠죠. 
즉 x를 변화시켜가며, 에러벡터의 norm을 최소화 시키고 싶은겁니다.
그리고 이 식이 그뜻이됩니다. 즉 x햇의 의미는, 에러벡터의 norm이 최소가 되는 x입니다. 그리고 이 x햇이 최적의 솔루션이 되는겁니다. 

추가적으로 생각해볼수 있는것이, Ax는 일단 A의 col 벡터들의 span에 존재하게됩니다. 즉 col A 에 존재하게됩니다.

##22-1 (20)
이번엔 이 least squares 를 기하학적으로 보겠습니다. A를 2개의 lin indep한 col벡터로 이루어져있다고 하면, 
col A는 평면이 됩니다. 그리고 b-Ax가 최소가 되어야하므로, 어떠한 x가 b-Ax의 norm 을 최소화 시키는지 생각해보면 됩니다.
근데 아까 b-Ax의 norm 은 거리였으니, 여기 수선의 발이 최소가 되는겁니다. 이는 그림에서도 보시다싶이 당연한것이죠.

결론적으로는, b 에서 이 평면에 수선의 발을 내렸을때, 그 점이 A x햇이 되는것입니다.
그럼 b-Ax햇 벡터를 생각해볼수있고 이 벡터는 평면과 수직이되어야합니다.
그럼 벡터가 평면과 수직이라는것은 무슨의미 일까요? 그 의미는 col의 모든 벡터와 수직이 되어야한다는 것입니다.

##22-2 (21)
그래서 이를 다시 보시면, 모든 A의 col 벡터의 linear combination과 수직이 되어야하는것이죠.
근데 이는 아래와 동치가 됩니다.
왜냐면 수직의 조건이 내적해서 0이 나오면 되는것이었는데, 분배법칙을 사용하는것입니다.
아래의 조건을 만족하면, (b-Ax햇)T * (a1) = 0이므로 (b-Ax햇)T * (a1*x1) = 0 이 됩니다. 
즉 이를 다시쓰면 AT * (b-Ax헷)를 0으로 만들어주는 x햇을 찾는겁니다.

##23-1 (22)
그래서 결론적으로 이 최종 식을 normal equation 라고 부릅니다.
이는 새로운 Cx=d의 선형방정식으로 볼수 있습니다. C는 n*n의 square매트릭스가 되고 d는 n차원의 벡터입니다.
또 C가 invertible, 즉 C-1를 구할수있다면 아래와 같이도 쓸수 있습니다.

##24-1 (23)
normal equation을 다른 방법으로도 유도를 할수있습니다.
여기 보시면, 벡터의 길이의 최솟값을 구하는건데, 그 최솟값을 구하는건 제곱의 최솟값을 구하는것과 같습니다. 왜냐면 양수이기 때문이죠.
그래서 이렇게 쓸 수 있는것이고, 이걸 분배법칙을 이용하면 이렇게 됩니다.
이를 미분하면 되는것입니다.

네 그전에 그래디언트에 대해 설명을 드리겠습니다. 그래디언트가 기울기 벡터인데, 그래서 0벡터가 되게하면, 최솟값을 구할수있는것이죠.
그럼 미분을 하기전에, 먼저 f(x)=3x1+5x2라고 해보겠습니다. 이를 벡터곱으로 나타내면, [3,5]*[x1,x2]가 되겠죠.
이걸 aTx 라고 하면, 그래디언트를 구하면, 그냥 a가 됩니다. 그래서 aTx 의 미분값이 a가 되는것이죠.
그럼 다시 xTa 라고 쓸수있고, 이를 미분하면 a가 되는것입니다.

그래서 이를 미분하면 이렇게 되는것입니다. 그러면 저희가아는 normal equation이 유도가 됐습니다.

##25-1 (24)
자그러면 이제 다시 예시로돌아와서 normal equation을 적용해봅시다
그럼 ATA가 이렇게 3*3매트릭스가 되는것이죠.

##25-2 (25)
invertible하지 않으면 어떻게 될까요? 해가 없거나 무수히 많게 됩니다. 
[12][36]  x = [5,10] [5,11] 이런경우를 생각해보시면 될것같습니다.
근데 해가 존재하지 않는 경우는 없습니다. 왜냐면 수선의 발을 A의 col벡터 span에 내릴수 있냐 없냐인데, 그런데 수선의 발은 항상 존재를 하겠죠. 
해는 반드시 존재하는데, 이게 유니크한 해인지, 무수히 많은 해인지는 어떻게 따질수있냐면, 바로 A의 열벡터의 
선형독립인지를 따져주면 됩니다. 애초에 저희가 이 해를 구한다는건, 수선의 발을 내리는건데, 그 수선의 발이 유니크하게
표현되는지를 확인하면 되는것입니다. 해가 유니크하다는건, lin indep하다는것이죠.
그런데 생각해보시면 A는 대부분 lin indep 합니다.
real data는 사람수가 많아질수록 lin indep하기 떄문이죠. 즉 feature보다 sample수가 많아지면, 높은확률로 lin indep한것입니다.
그럼 다른 측면에서 생각해봅시다. 이렇게 많은 샘플들이 있으면, 적은 갯수의 벡터들로 span이 되니까, 예를들어 100차원 벡터 두개가 있으면 100차원을 span하지 못하고 
평면이 되는것이죠. 전체공간 대비 span하는 공간은 일부에 불과한것이죠. 그러면 에러가 커질수밖에 없는것입니다.

그래서 결론적으로 C가 invertible하면, A가 lin indep한겁니다. 그리고 유니크한 해를 갖는다는건, 수선의 발이 유니크하게 표현이 되는것이죠.
그래서 C가 invertible 하다는건 유니크한 해를 갖는것이죠 

##26-1 (26)
그러면 이번엔 orthogonal projection에 대해 알아보겠습니다.
orthogonal projection이란, 수직으로 내린 점의 벡터라고 생각하시면 됩니다. 그래서 다시 이 least squares에서,
b를 이제 col A에 수선의발을 내린 그 점이되는것이고, 그점은 Ax햇 이되는것이죠 그리고 ATA가 invertible하면, 
최종적으로 b의 orthogonal projection을 이렇게 쓸수 있습니다. 근데 잘보시면 이게 바로 b의 linear combination인것이죠.
이제 저희는 A가 orthonormal한 경우를 살펴보겠습니다.

##26-2 (27)
다음은 orthogonal set입니다. 간단합니다. 모든 벡터의 내적이 0이면 됩니다. 
orthonormal set은 모든 벡터가 유닛벡터인 orthogonal set입니다
그리고 orthogonal set은 항상 lin indep합니다. -> c1v1+c2v2.. = 0 에서 양변에 v1다시 곱하면 c1=0

##26-3 (28)
다음은 그램슈미트 프로세스 입니다. 그램슈미트 프로세스는, lin indep한 벡터들을 lin indep 한성질을 만족하면서, 
같은 subspace를 span하고 서로 수직인 벡터로 만드는것입니다.

그럼 그램슈미트 프로세스를 하는 이유는 무엇인지 생각해보는겁니다. 먼저 b의 orthogonal projection에서 다시 생각해봅시다.
중간에보시면 ATA의 역행렬을 구해야하는데, 이 과정이 매우 오래걸리게 됩니다. 그래서 A의 각 열벡터를 그램슈미트과정을 거치면, 
새로운 u 행렬이 나오는데, A를 u로 바꿔줘도 답은 같습니다. 왜냐? span하는 서브스페이스가 같기때문입니다.
그럼 uTu의 역행렬을 직접 구해보겠습니다. 이는 서로다른 벡터의 내적이 0 임을 이용해 쉽게 구할수있게 됩니다. 
즉 그램슈미트과정을 거치면, b의 orthogonal projection을 쉽게 구할수있게되는겁니다. 
만약 orthonormal하게 그램슈미트과정을 거쳤다면, 같은벡터끼리의 내적이 1 이므로, uTu가 그냥 아이덴티티 매트릭스가 되는것이죠.

##26-4 (29)
그럼 그램슈미트 프로세스의 수식을 살펴보겠습니다.
이 그림에서, u와 y에 대해서 그램슈미트 프로세스를 적용한다고 하면, 그 의미는 서로 orthogonal 하면서, 같은 span을 갖는 두 선형독립인 벡터를 찾는겁니다.
먼저 u는 고정시킵니다. 그럼 u에 수직이면서, 새로운 벡터 y-y햇 을 찾는겁니다. 여기서 orthogonal projection을 사용할 수 있습니다.
즉 벡터u가 만드는 span, 즉 직선에 y의 orthogonal projection을 y햇 이라하면, y-y햇은 u에 orthogonal 한 벡터가 되는것입니다.

그럼 이를 어떻게 구할수있을까요? 먼저 u를 normalize합니다. 즉 u를 1/||u|| * u 라고 쓸수있죠 그러면 uTu가 아까 말씀드린대로 1이 되면서, y햇은 그냥
1/||u||^2 * uuT y 가 됩니다. uuT y 는 다시 어떻게 쓸 수 있냐면, uTy 는 row 와 col 의 내적이기때문에, 상수가됩니다.
즉 이 상수를 앞으로 빼서 생각하면됩니다. 그리고 uTy는 u와 y의 내적입니다. 그래서 결론적으로 y햇은 요 u의 linear combination으로 나타내지는겁니다.

이번엔 다르게 y햇을 유도를 해보겠습니다. 사이각을 세타라고하면, |y|cos세타가 y햇의 길이가 됩니다. 그러므로
y햇은 ||y||cos세타에 u/||u||를 곱한것이 되겠죠. 근데 cos세타가 y와 u의 내적/||y||*||u||이므로 결국 
똑같게 결론이 나옵니다.

##26-5 (30)
이번엔 평면으로의 orthogonal projection입니다. 이는 두개의 벡터의 합으로 나타낼수 있고, 
그 벡터를 y1햇, y2햇 이라고해보겠습니다. 그럼 각각 벡터는 y의 라인으로의 orthogonal projection입니다.
그래서 이렇게 나타낼수 있는겁니다. 앞의 나오는 항이 y1햇, 뒷항이 y2햇 입니다.

##26-6 (31)
그럼실제 그램슈미트과정을 예시로 알아보겠습니다.

##26-7 (32)
먼저 v1은 x1으로 둡니다. 이후 v2를 구해주면됩니다. v2는 x2에서 x2의 orthogonal projection을 뺴준값이 되겠죠.

##26-8 (33)
v3도 마찬가지입니다. 

##26-9 (34)
이를 그림으로 보시면 x3에서 x3의 orthogonal projection을 뺴준것이 v3가 되는겁니다.

##26-10 (35)
다음은 QRfactorization입니다. 매트릭스 A를 QR의 행렬곱으로 나타내는건데, Q의 column들이 이제 그램슈미트 과정을 거친 벡터들이 됩니다.

##26-11 (36)
네 근데 R을 구하는건 어렵지 않습니다. 바로 이 행렬곱을 column벡터들의 linear combination으로 보는겁니다.
근데 u1은 A의 첫번째 벡터, x1과 span하는 직선이 같으므로, R의 첫번쨰 벡터는 2,0,0 이 됩니다.
다음 두번째 R의 열벡터를 구하기위해, x2를 u1과 u2와 u3의 선형결합으로 생각해봅시다. 근데 u2가 x2-( (x2와u1내적) / ||u1||^2 ) * u1의 유닛벡터 이므로
그래서 요 노멀라이즈값을 3이라고하면, 여기서 x2를 3 * u2 + (x2u1내적) * u1으로 다시 나타낼수 있습니다. 그러면 원하는대로 x2를 u1, u2의 선형결합으로 나타냈으므로,
R의 두번째 벡터는 x2,u1내적, 3, 0이 됩니다. 
마찬가지로 u3 를 x3 - x3내적u1 * u1 -  x3내적u2 * u2 이므로, R의 마지막 벡터또한 x3내적u1, x3내적u2, 노멀라이즈값 이렇게 됩니다.

##26-12 (37)
네 그래서 QRfactorization이 결론적으로 이렇게 나옵니다.