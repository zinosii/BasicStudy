
#script

##intro
안녕하세요. 이전시간에 이어서 계속 발표 진행하겠습니다.

##19-1 (1)
이번에 알아볼개념은 요 over determined linear system 에서의 해 입니다
저희가 알다싶이 변수의 갯수보다 방정식의 갯수가 많으면, 일반적으로 해가 존재하지 않겠죠
이때 해를 근사적으로 찾아보는것이, least squares입니다.

##19-2,3 (2,3)
말씀드린대로 방정식, equations가 많아야 하므로. 요 3*3 매트릭스를 100*3으로 확장 시켜봅시다. 그럼 b벡터 또한 100차원 벡터가 되겠죠.
그리고 매트릭스는, 100차원 col벡터 3개가 되는것이죠. 여기에 3*1벡터를 곱해주면, 100차원을 커버하기엔 무리가 있는것이죠.
대부분의 b는 스팬 밖에 존재해서 해가 없게 되는겁니다.

##19-4 (4)
그럼 근사적으로 해를 찾으려는데, 근사적인 해가 무엇인지 알아봐야합니다.

##19-5 (5)
그전에 기본적인 개념을 알아봅시다. 
먼저 inner product입니다.
두 벡터의 내적, 즉 u * v 이렇게 쓸 수 있고, uT * v 라고 생각하시면 됩니다.
모두 아시다싶이 결과는 스칼라 값이 나옵니다.

##19-6 (6)
내적의 성질인데요, 교환법칙과 분배법칙이 성립합니다. 
그리고 요 상수c를 먼저곱하든, 나중에 한번에 곱하든 결과는 같습니다.
마지막 성질을 보기전에, 벡터의 길이먼저 설명드리겠습니다. 벡터 u가 있을때, u의 길이를 저희는 루트 u*u이라고 하고, ||u||라고 쓸수 있습니다.
그럼 uT * u 가 길이의 제곱이 되겠죠. 그럼이제 마지막 성질을 이해 하실 수 있을것 같습니다. 자기자신과 내적해서 0이기때문에,
그 벡터는 무조건 0이상이고, 0인경우는 무조건 0벡터가 되어야하는것입니다.

그리고 마지막 성질은, b,c성질을 이용한것입니다.

##20-1 (7)
네그럼 아까 설명드린 길이, 그 길이를 이제는 norm이라고 합니다.

##20-2 (8)
이 norm을 좀 기하학적으로 생각해보면, 왜 길이라고 하는지 알수 있으실겁니다.

##20-3 (9)
다음은 단위 벡터인데, 단위 벡터는 길이가 1인 벡터입니다.
여기 벡터 노멀라이징이라고 되어있는데, 벡터를 단위 벡터로 만들어 주는 과정이라고 생각하시면 됩니다.
즉 벡터 v를 norm으로 나눠주는것입니다. 그러면 방향은 같으나, 길이는 당연히 1이됩니다.

##20-4 (10)
그러면 이번엔 벡터간의 거리를 정의해보겠습니다.
이걸 좌표평면에서 보시면 이해가 빠를것같은데, 요 좌표평면상 두개의 벡터 u,v가 있다고 했을때, u,v사이의 거리를 정의해보면,
||u-v||가 되는데, 이를 u+(-v) 이렇게 생각해볼수 있고 이를 좌표평면에 나타내면 이 벡터가 되는것입니다. 요 평행사변형을보시면 이해가 빠르실겁니다.

##20-5 (11)
Rn에서도 마찬가지 입니다.

##20-6 (12)
다음은 두 벡터사이의 각도입니다.
두 벡터의 내적은, 두벡터의 norm의 곱과 두벡터사이 각의 코사인값의 곱이됩니다.
즉 cos theta 는 내적값을 norm의 곱으로 나눠준것과 같은것이죠.

##20-7 (13)
그럼 두개의 벡터가 수직인지를 이를 이용해 알 수 있겠죠.
내적해서 0이된다는 뜻은, 즉 코사인값이 0이되야하므로, 두벡터사이각이 90도, 즉 수직인 경우 입니다.
그래서 내적해서 0인지만 판단하면 되는것이죠. 물론 non zero 벡터여야 합니다.

##21-1 (14)
그럼 다시 아까의 방정식으로 돌아와볼게요.
우리가 이 방정식의 해를 이렇게 구했다고 해봅시다.

##21-2 (15)
그 후에, 새로운 네번째 식을 추가하는겁니다.
그러면 Ax=b 에서 A가 4*3 으로 바뀌겠죠. 또한 결과 값도 4*1 벡터로 바뀌게 됩니다. 그러면 이 마지막 원소는, 72와 같다는 보장이 없습니다.
즉 에러가 발생합니다. 왜냐면 x가 마지막 방정식에 대해서는 고려하지 않은 솔루션이기 때문입니다. 
그리고 에러는, b-Ax라고 할 수 있습니다.

##21-3 (16)
그럼 이번엔 다른 솔루션을 대입해보겠습니다.
그러면 저희 모델에 예측치는 이렇게 나오겠죠. 에러도 당연히 달라지는겁니다.

##21-4 (17)
그럼 더 나은 솔루션은 뭐가 될지 생각해볼수있겠죠.

##21-5 (18)
그걸 판단하기 위해 요 새로운 식, sum of squared errors 를 생각해볼수있습니다.
위의 에러가 더 작기때문에, 위의 솔루션이 더 나은 솔루션이라고 할 수 있습니다.
그리고 이식은 b-Ax의 norm입니다.

##21-6 (19)
그럼 이 에러를 최소화 하는것을 생각할수 있겠죠. 
즉 x를 변화시켜가며, 에러벡터의 norm을 최소화 시키고 싶은겁니다.
그리고 이 식이 그뜻이됩니다. 즉 x햇의 의미는, 에러벡터의 norm이 최소가 되는 x입니다. 그리고 이 x햇이 최적의 솔루션이 되는겁니다. 

추가적으로 생각해볼수 있는것이, Ax는 일단 A의 col 벡터들의 span에 존재하게됩니다. 즉 col A 에 존재하게됩니다.

##22-1 (20)
이번엔 이 least squares 를 기하학적으로 보겠습니다. A를 2개의 lin indep한 col벡터로 이루어져있다고 하면, 
col A는 평면이 됩니다. 그리고 b-Ax가 최소가 되어야하므로, 어떠한 x가 b-Ax의 norm 을 최소화 시키는지 생각해보면 됩니다.
근데 아까 b-Ax의 norm 은 거리였으니, 여기 수선의 발이 최소가 되는겁니다. 이는 그림에서도 보시다싶이 당연한것이죠.

결론적으로는, b 에서 이 평면에 수선의 발을 내렸을때, 그 점이 A x햇이 되는것입니다.
그럼 b-Ax햇 벡터를 생각해볼수있고 이 벡터는 평면과 수직이되어야합니다.
그럼 벡터가 평면과 수직이라는것은 무슨의미 일까요? 그 의미는 col의 모든 벡터와 수직이 되어야한다는 것입니다.

##22-2 (21)
그래서 이를 다시 보시면, 모든 A의 col 벡터의 linear combination과 수직이 되어야하는것이죠.
근데 이는 아래와 동치가 됩니다.
왜냐면 수직의 조건이 내적해서 0이 나오면 되는것이었는데, 분배법칙을 사용하는것입니다.
아래의 조건을 만족하면, (b-Ax햇)T * (a1) = 0이므로 (b-Ax햇)T * (a1*x1) = 0 이 됩니다. 
즉 이를 다시쓰면 AT * (b-Ax헷)를 0으로 만들어주는 x햇을 찾는겁니다.

##23-1 (22)
그래서 결론적으로 이 식을 normal equation 라고 부릅니다.
이는 새로운 Cx=d의 선형방정식으로 볼수 있습니다. C는 n*n의 square매트릭스가 되고 d는 n차원의 벡터입니다.
또 C가 invertible, 즉 C-1를 구할수있다면 아래와 같이도 쓸수 있습니다.
